{"componentChunkName":"component---src-templates-note-js","path":"/nltk/","result":{"data":{"remark":{"fields":{"path":"nltk"},"meta":{"title":""},"headings":[{"value":"NLTK samples"}],"html":"<h1>NLTK samples</h1>\n<h2>Installation</h2>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token function\">install</span> python-dev\n<span class=\"token function\">sudo</span> pip <span class=\"token function\">install</span> <span class=\"token parameter variable\">-U</span> numpy\n<span class=\"token function\">sudo</span> pip <span class=\"token function\">install</span> <span class=\"token parameter variable\">-U</span> pyyaml nltk</code></pre></div>\n<p>For windows there are msi installers, follow links from here: <a href=\"http://nltk.org/install.html\">http://nltk.org/install.html</a></p>\n<p>To check installation: run <code class=\"language-text\">python</code> then type <code class=\"language-text\">import nltk</code></p>\n<p>Download dictionaries:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">python <span class=\"token parameter variable\">-m</span> nltk.downloader all</code></pre></div>\n<p>Now you can check that all installed:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> nltk<span class=\"token punctuation\">.</span>corpus <span class=\"token keyword\">import</span> brown\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> brown<span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">[</span><span class=\"token string\">'The'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Fulton'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'County'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Grand'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Jury'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'said'</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span></code></pre></div>\n<h2>Stemming</h2>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> nltk <span class=\"token keyword\">import</span> stem\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stem<span class=\"token punctuation\">.</span>SnowballStemmer<span class=\"token punctuation\">.</span>languages\n<span class=\"token punctuation\">(</span><span class=\"token string\">'danish'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'dutch'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'english'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'finnish'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'french'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'german'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'hungarian'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'italian'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'norwegian'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'porter'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'portuguese'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'romanian'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'russian'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'spanish'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'swedish'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> russian_stemmer <span class=\"token operator\">=</span> stem<span class=\"token punctuation\">.</span>SnowballStemmer<span class=\"token punctuation\">(</span><span class=\"token string\">'russian'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">print</span> russian_stemmer<span class=\"token punctuation\">.</span>stem<span class=\"token punctuation\">(</span><span class=\"token string\">u'киева'</span><span class=\"token punctuation\">)</span>\nкиев\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stemmer <span class=\"token operator\">=</span> stem<span class=\"token punctuation\">.</span>PorterStemmer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stemmer<span class=\"token punctuation\">.</span>stem<span class=\"token punctuation\">(</span><span class=\"token string\">'buying'</span><span class=\"token punctuation\">)</span>\n<span class=\"token string\">'buy'</span></code></pre></div>\n<h2>Tokenize</h2>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">import</span> nltk\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> tokenizer <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">.</span>RegexpTokenizer<span class=\"token punctuation\">(</span><span class=\"token string\">r'\\w+|[^\\w\\s]+'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> tagger <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>UnigramTagger<span class=\"token punctuation\">(</span>nltk<span class=\"token punctuation\">.</span>corpus<span class=\"token punctuation\">.</span>brown<span class=\"token punctuation\">.</span>tagged_sents<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> tokenized <span class=\"token operator\">=</span> tokenizer<span class=\"token punctuation\">.</span>tokenize<span class=\"token punctuation\">(</span><span class=\"token string\">\"Hello World! Mac was here.\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">print</span> tokenized\n<span class=\"token punctuation\">[</span><span class=\"token string\">'Hello'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'World'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'!'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Mac'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'was'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'here'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'.'</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> tagged <span class=\"token operator\">=</span> tagger<span class=\"token punctuation\">.</span>tag<span class=\"token punctuation\">(</span>tokenized<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">print</span> tagged\n<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Hello'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'UH'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'World'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'NN-TL'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'!'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'.'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'Mac'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'NP'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'was'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'BEDZ'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'here'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'RB'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'.'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'.'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>"}},"pageContext":{"id":"0110e9da-03c3-5dc4-800c-00301bd1824c"}},"staticQueryHashes":[],"slicesMap":{}}